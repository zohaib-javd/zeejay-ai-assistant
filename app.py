import streamlit as st
import time
from langchain_ollama import OllamaLLM

# =======================
# Page Configuration
# =======================
st.set_page_config(
    page_title="Zeejay Labs AI Assistant",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =======================
# Session State for Chat History
# =======================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "total_messages" not in st.session_state:
    st.session_state.total_messages = 0

# =======================
# Sidebar
# =======================
with st.sidebar:
    st.title("Zeejay Labs")
    st.markdown("ğŸ¤– Local AI Assistant")
    st.markdown("âš¡ Powered by Llama 3")
    st.markdown("ğŸ”’ Privacy First")

    st.markdown("---")
    st.subheader("System Status")
    st.markdown(f"ğŸ’¬ **Total Messages:** {st.session_state.total_messages}")
    st.markdown(f"ğŸ“ **Current Session:** {len(st.session_state.messages)} messages")

    st.markdown("---")
    if st.button("ğŸ§¹ Clear Chat History"):
        st.session_state.messages = []
        st.success("Chat history cleared!")
        time.sleep(1)
        st.rerun()

    st.markdown("---")
    st.subheader("ğŸ’¡ Tips")
    st.info("â€¢ Ask complex questions\nâ€¢ Request code examples\nâ€¢ Get explanations\nâ€¢ Brainstorm ideas")

    st.markdown("---")
    st.markdown("Built with â¤ï¸ using Streamlit + LangChain + Ollama")

# =======================
# Main App Content
# =======================
st.title("Zeejay's AI Assistant")

if len(st.session_state.messages) == 0:
    st.markdown(
        """
ğŸ‘‹ **Welcome to your Personal AI Assistant!**

I'm powered by Llama 3 and running locally on your machine. Ask me anythingâ€”from coding help to creative writing, analysis, or just casual conversation!

**ğŸš€ Ready to get started? Type your message below!**
        """
    )

# =======================
# Initialize Ollama LLM
# =======================
@st.cache_resource
def load_llm():
    try:
        return OllamaLLM(model="llama3")
    except Exception as e:
        st.error(f"Error loading AI model: {str(e)}")
        st.info("Make sure Ollama is running and the llama3 model is installed.")
        return None

llm = load_llm()

# =======================
# Chat History
# =======================
for msg in st.session_state.messages:
    with st.chat_message(msg["role"], avatar="ğŸ‘¤" if msg["role"] == "user" else "ğŸ§ "):
        st.markdown(msg["content"])

# =======================
# Chat Input
# =======================
if llm is not None:
    prompt = st.chat_input("ğŸ’­ Ask me anything... (Press Enter to send)")

    if prompt:
        st.session_state.total_messages += 1
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar="ğŸ§ "):
            with st.spinner("ğŸ¤” Thinking..."):
                try:
                    response = llm.invoke(prompt)
                except Exception as e:
                    response = f"Sorry, I encountered an error: {str(e)}"
                    st.error(response)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

        st.rerun()
else:
    st.error("âš ï¸ AI Model not available. Please check your Ollama installation.")
    st.info(
        """
To fix this issue:
1. Make sure Ollama is installed and running
2. Run: `ollama pull llama3`
3. Restart this application
        """
    )
